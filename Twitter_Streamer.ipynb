{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#tweepy functions used to stream tweets\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Import credentials for the user\n",
    "from twitter_credentials import KEY, SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    "\n",
    "#textblob will be the library that analizes the sentiments\n",
    "from textblob import TextBlob\n",
    "\n",
    "import json\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define streaming classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Twitter Streaner\n",
    "class TwitterStreamer():\n",
    "    \"\"\"\n",
    "    Class for streaming and processing tweets.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def stream_tweets(self, fetched_tweets_filename, hash_tag_list,time_span,save_data):\n",
    "        # Listener class will peform the processing of the live tweets\n",
    "        listener = Listener(fetched_tweets_filename,time_span,save_data)\n",
    "\n",
    "        #This section handles authentication to use Twitter API  \n",
    "        auth = OAuthHandler(KEY, SECRET)\n",
    "        auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "        #Stream will be the executer having authentificated and defined processing\n",
    "        stream = Stream(auth, listener)\n",
    "\n",
    "        # Filter Streams to capture data by keywords \n",
    "        stream.filter(languages = ['en'], track = hash_tag_list)\n",
    "\n",
    "\n",
    "#TWITTER STREAM LISTENER \n",
    "class Listener(StreamListener):\n",
    "    \"\"\"\n",
    "    Class for processing tweets.\n",
    "    \"\"\"\n",
    "    def __init__(self, fetched_tweets_filename,time_span,save_data):\n",
    "        self.fetched_tweets_filename = fetched_tweets_filename\n",
    "        super().__init__()\n",
    "        self.time_span = time_span #Time window that the listener will be streaming\n",
    "        self.save_data = save_data #Will specify if the streamed tweets will be saved\n",
    "        self.started = time.time() \n",
    "\n",
    "    #Process data:\n",
    "    def on_data(self, data):\n",
    "            try:\n",
    "                starting = time.time()\n",
    "                data = json.loads(data)\n",
    "                #Get elements from fetched tweet info:\n",
    "                text = data['text']\n",
    "                user = data['user']\n",
    "                retweets = data['retweet_count']\n",
    "                likes = data['favorite_count']\n",
    "                sentiment = TextBlob(text)\n",
    "\n",
    "                tweet ={\"user\":user['screen_name'], \"followers\":user['followers_count'],\"text\" : text, \"sentiment\": sentiment.polarity, \"subjetivity\": sentiment.subjectivity}\n",
    "                tweet = json.dumps(tweet)\n",
    "                #save data on a external file:\n",
    "                if self.save_data == True:\n",
    "                    with open(self.fetched_tweets_filename, 'a') as tf:\n",
    "                        tf.write(data)\n",
    "\n",
    "                #Send processed data to Kafka topic\n",
    "                producer.send('tweets', key = b'tweet', value = str(tweet))\n",
    "\n",
    "                if starting - self.started <= self.time_span:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "            except BaseException as e:\n",
    "                print(\"Error on_data %s\" % str(e))\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "        if status == 420:\n",
    "            #returning False if on_data disconnects the stream\n",
    "            return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Kafka producer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create kafka server and tweets topic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ZooKeeper Server\n",
    "!./kafka_2.13-2.8.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-2.8.0/config/zookeeper.properties\n",
    "# Kafka Server\n",
    "!./kafka_2.13-2.8.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-2.8.0/config/server.properties"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create tweets topic in kafka server\n",
    "!./kafka_2.13-2.8.0/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic tweets\n",
    "#Add a time limit for data to stay loaded in tweets topic\n",
    "!./kafka_2.13-2.8.0/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic tweets --config retention.ms=1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define producer and beggin streaming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from kafka import KafkaProducer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Create producer and specify the value encoding\n",
    "producer = KafkaProducer(value_serializer = lambda x: x.encode('utf-8'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Stream tweets and send them to kafka topic\n",
    "hash_tag_list = ['etherum','bitcoin','dogecoin']\n",
    "fetched_tweets_filename = \"ETH_BIT_DOGE_tweets.txt\"\n",
    "\n",
    "twitter_streamer = TwitterStreamer()\n",
    "twitter_streamer.stream_tweets(fetched_tweets_filename, hash_tag_list,300,save_data= False)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  }
 ]
}